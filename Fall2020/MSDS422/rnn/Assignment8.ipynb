{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries needed for the analysis\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "import random\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed value for random number generators to obtain reproducible results\n",
    "RANDOM_SEED = 5\n",
    "from numpy.random import seed\n",
    "seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training and test datasets\n",
    "train_tweets = pd.read_csv(\"/Users/kerry//Projects/grad_school_portfolio/Fall2020/MSDS422/rnn/data/train.csv\")\n",
    "test_tweets = pd.read_csv(\"/Users/kerry//Projects/grad_school_portfolio/Fall2020/MSDS422/rnn/data/test.csv\")\n",
    "#train_tweets= pd.read_csv('/Users/harini-mac/Desktop/Northwestern University/MSDS-422/Week8/project1/nlp-getting-started/train.csv')\n",
    "#test_tweets=pd.read_csv('/Users/harini-mac/Desktop/Northwestern University/MSDS-422/Week8/project1/nlp-getting-started/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train_tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3263 entries, 0 to 3262\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        3263 non-null   int64 \n",
      " 1   keyword   3237 non-null   object\n",
      " 2   location  2158 non-null   object\n",
      " 3   text      3263 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 102.1+ KB\n"
     ]
    }
   ],
   "source": [
    "test_tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7604</th>\n",
       "      <td>10863</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#WorldNews Fallen powerlines on G:link tram: U...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7605</th>\n",
       "      <td>10864</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>on the flip side I'm at Walmart and there is a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7606</th>\n",
       "      <td>10866</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Suicide bomber kills 15 in Saudi security site...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7503 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7604  10863     NaN      NaN   \n",
       "7605  10864     NaN      NaN   \n",
       "7606  10866     NaN      NaN   \n",
       "7608  10869     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7604  #WorldNews Fallen powerlines on G:link tram: U...       1  \n",
       "7605  on the flip side I'm at Walmart and there is a...       1  \n",
       "7606  Suicide bomber kills 15 in Saudi security site...       1  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7503 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drops dups (retweets)\n",
    "train_tweets[train_tweets.duplicated(subset=\"text\", keep='first')==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>938</td>\n",
       "      <td>blaze</td>\n",
       "      <td>Temecula, CA</td>\n",
       "      <td>Pendleton media office said only fire on base ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>940</td>\n",
       "      <td>blaze</td>\n",
       "      <td>Fresno, CA</td>\n",
       "      <td>Love living on my own. I can blaze inside my a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>945</td>\n",
       "      <td>blaze</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Property losses from California wildfire nearl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>946</td>\n",
       "      <td>blaze</td>\n",
       "      <td>Raleigh Durham, NC</td>\n",
       "      <td>#breaking Firefighters battling blaze at east ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>964</td>\n",
       "      <td>blaze</td>\n",
       "      <td>Karachi</td>\n",
       "      <td>Property losses from #California wildfire near...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id keyword            location  \\\n",
       "647  938   blaze        Temecula, CA   \n",
       "649  940   blaze          Fresno, CA   \n",
       "653  945   blaze           Australia   \n",
       "654  946   blaze  Raleigh Durham, NC   \n",
       "669  964   blaze            Karachi    \n",
       "\n",
       "                                                  text  target  \n",
       "647  Pendleton media office said only fire on base ...       1  \n",
       "649  Love living on my own. I can blaze inside my a...       1  \n",
       "653  Property losses from California wildfire nearl...       1  \n",
       "654  #breaking Firefighters battling blaze at east ...       1  \n",
       "669  Property losses from #California wildfire near...       1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drops ~ 5-10 records most of which I find misclassified\n",
    "train_tweets[(train_tweets.keyword==\"blaze\") & (train_tweets.target==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 0 entries\n",
      "Data columns (total 11 columns):\n",
      " #   Column                                      Non-Null Count  Dtype \n",
      "---  ------                                      --------------  ----- \n",
      " 0   Model_name                                  0 non-null      object\n",
      " 1   vocabulary size (factor1)                   0 non-null      object\n",
      " 2   pre-trained vector (factor2)                0 non-null      object\n",
      " 3   RNN structure (factor3)                     0 non-null      object\n",
      " 4   hyperparameter recurrent_dropout (factor4)  0 non-null      object\n",
      " 5   Processing Time                             0 non-null      object\n",
      " 6   Training Set Accuracy                       0 non-null      object\n",
      " 7   Validation Set Accuracy                     0 non-null      object\n",
      " 8   train f1                                    0 non-null      object\n",
      " 9   validation f1                               0 non-null      object\n",
      " 10  Test Set F1-score (Kaggle score)            0 non-null      object\n",
      "dtypes: object(11)\n",
      "memory usage: 0.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "#Create a dataframe to store the model accuracy and scores\n",
    "results_tbl = pd.DataFrame(columns=['Model_name','vocabulary size (factor1)', 'pre-trained vector (factor2)','RNN structure (factor3)','hyperparameter recurrent_dropout (factor4)','Processing Time','Training Set Accuracy','Validation Set Accuracy','train f1','validation f1','Test Set F1-score (Kaggle score)'])\n",
    "results_tbl.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis & Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'target count plot in the training data (0 - not a real disaster, 1 - a real disaster)')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count plot of the targets in training data \n",
    "sns.countplot(train_tweets['target'], palette=\"husl\")\n",
    "plt.title('target count plot in the training data (0 - not a real disaster, 1 - a real disaster)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       61\n",
       "location    2533\n",
       "text           0\n",
       "target         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for any null values \n",
    "# the variable of interest in the data is text.\n",
    "train_tweets.isna().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Distribution of the length of the tweets in the training data\n",
    "#fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n",
    "#disaster_tweets_len=train_tweets[train_tweets['target']==1]['text'].str.len()\n",
    "#nondisaster_tweets_len=train_tweets[train_tweets['target']==0]['text'].str.len()\n",
    "#sns.histplot(disaster_tweets_len, ax=ax1, color=\"g\")\n",
    "#ax1.set_title('Length of the disaster tweets in training data')\n",
    "#sns.histplot(nondisaster_tweets_len, ax=ax2, color=\"r\")\n",
    "#ax2.set_title('Length of the non-disaster tweets in training data')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Distribution of the word lengths of the tweets in the  training data\n",
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n",
    "diaster_tweet_lengths=train_tweets[train_tweets['target']==1]['text'].str.split().map(lambda x: len(x))\n",
    "ax1.hist(diaster_tweet_lengths,color='blue')\n",
    "ax1.set_title('word length distribution of \\n disaster tweets in training data')\n",
    "nondisaster_tweet_lengths=train_tweets[train_tweets['target']==0]['text'].str.split().map(lambda x: len(x))\n",
    "ax2.hist(nondisaster_tweet_lengths,color='yellow')\n",
    "ax2.set_title('word length distribution of \\n non-disaster tweets in training data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read function for the  GloVe (global vectors) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading from the file to turn the words to word embedding vector\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Glove file - glove.6B.50d.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading from the file to learn the word embedding into the list word_to_vec_map\n",
    "#word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('/Users/harini-mac/Desktop/Northwestern University/MSDS-422/Week8/project1/nlp-getting-started/embeddings/gloVe.6B/glove.6B.50d.txt')\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('/Users/kerry/Downloads/embeddings/gloVe.6B/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_vec_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanse the training and test data \n",
    "## Use the word_to_vec_map from glove.6B.50d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "#    rgx_fix_caps = \"(?<=[\\.!?]\\s)([A-Z])\" # fixes like Hello into hello???\n",
    "#    subst = r\"\\\\\"\n",
    "#    pre_sentence = re.sub(rgx_fix_caps, subst, text)\n",
    "    \n",
    "    rgx_rmv_punc = re.compile('([^\\s\\w]|_)+') # removes punctuation\n",
    "    sentence = rgx_rmv_punc.sub(\"\", text).lower()\n",
    "    \n",
    "    sentence = sentence.split(\" \")\n",
    "    \n",
    "    for word in list(sentence):\n",
    "        if word not in word_to_vec_map: # removes word if it's not in the 'dictionary' (txt file). case-sensitive.\n",
    "            sentence.remove(word)\n",
    "            \n",
    "    sentence = \" \".join(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'all residents asked to shelter in place are being notified by officers no other evacuation or shelter in place orders are expected'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean(train_tweets.loc[2,'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (train_tweets.shape[0]):\n",
    "    train_tweets.at[i,'text']=clean(train_tweets.loc[i,'text'])\n",
    "    \n",
    "for i in range(test_tweets.shape[0]):\n",
    "    test_tweets.at[i,'text']=clean(test_tweets.loc[i,'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train_tweets['text']\n",
    "test_text = test_tweets['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode the text in the training data\n",
    "# Pad the resultant sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/57693333/processing-before-or-after-train-test-split\n",
    "# https://www.kdnuggets.com/2020/03/tensorflow-keras-tokenization-text-data-prep.html\n",
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[178, 42, 214, 664, 6037, 6038, 1319]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([\"Forest fire near La Ronge Sask. Canada\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['forest fire near la ronge sask canada']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[178, 42, 214, 664, 6037, 6038, 1319]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens :  12256\n"
     ]
    }
   ],
   "source": [
    "word2index = tokenizer.word_index\n",
    "print(\"Number of unique tokens : \",len(word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7613"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_size = tokenizer.document_count\n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get max training sequence length\n",
    "maxlen = max([len(x) for x in train_sequences])\n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 31)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_seq = keras.preprocessing.sequence.pad_sequences(train_sequences,maxlen)\n",
    "X_train = padded_seq\n",
    "Y_train = train_tweets['target']\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate embedding matrix for tokens in the training data - 50 Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word2index)+1,50))\n",
    "\n",
    "embedding_vec=[]\n",
    "for word, i in word2index.items():\n",
    "    embedding_vec = word_to_vec_map.get(word)\n",
    "    if embedding_vec is not None:\n",
    "        embedding_matrix[i] = embedding_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12257, 50)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit RNN 1\n",
    "# vocabulary size (factor 1): 400K\n",
    "# pre-trained vector (factor 2): GloVe.6B.50d\n",
    "# rnn structure (factor 3): LSTM layer based structure\n",
    "# recurrent dropout (factor 4): 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = keras.models.Sequential([\n",
    "    keras.layers.Embedding(len(word2index)+1,50,weights=[embedding_matrix],input_length=maxlen,trainable=False),\n",
    "    keras.layers.LSTM(units=128 , return_sequences = True , recurrent_dropout = 0.1 , dropout = 0.1),\n",
    "    keras.layers.LSTM(units=64 , recurrent_dropout = 0.1 , dropout = 0.1),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(1,activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 31, 50)            612850    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 31, 128)           91648     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 756,019\n",
      "Trainable params: 143,169\n",
      "Non-trainable params: 612,850\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=Adam(learning_rate=1e-5)\n",
    "model1.compile(\n",
    "  loss='binary_crossentropy',\n",
    "  optimizer=opt,\n",
    "  metrics=['accuracy', f1_m],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "167/167 [==============================] - 18s 111ms/step - loss: 0.6899 - accuracy: 0.5294 - f1_m: 0.3271 - val_loss: 0.6805 - val_accuracy: 0.6038 - val_f1_m: 0.2494\n",
      "Epoch 2/40\n",
      "167/167 [==============================] - 19s 112ms/step - loss: 0.6699 - accuracy: 0.6690 - f1_m: 0.4568 - val_loss: 0.6573 - val_accuracy: 0.7172 - val_f1_m: 0.4316\n",
      "Epoch 3/40\n",
      "167/167 [==============================] - 21s 124ms/step - loss: 0.6480 - accuracy: 0.7247 - f1_m: 0.5869 - val_loss: 0.6306 - val_accuracy: 0.7680 - val_f1_m: 0.5311\n",
      "Epoch 4/40\n",
      "167/167 [==============================] - 23s 140ms/step - loss: 0.6213 - accuracy: 0.7607 - f1_m: 0.6714 - val_loss: 0.5977 - val_accuracy: 0.7728 - val_f1_m: 0.5542\n",
      "Epoch 5/40\n",
      "167/167 [==============================] - 23s 140ms/step - loss: 0.5902 - accuracy: 0.7630 - f1_m: 0.6896 - val_loss: 0.5627 - val_accuracy: 0.7863 - val_f1_m: 0.5835\n",
      "Epoch 6/40\n",
      "167/167 [==============================] - 24s 141ms/step - loss: 0.5589 - accuracy: 0.7664 - f1_m: 0.6967 - val_loss: 0.5315 - val_accuracy: 0.7828 - val_f1_m: 0.5968\n",
      "Epoch 7/40\n",
      "167/167 [==============================] - 24s 145ms/step - loss: 0.5354 - accuracy: 0.7699 - f1_m: 0.7048 - val_loss: 0.5127 - val_accuracy: 0.7815 - val_f1_m: 0.5870\n",
      "Epoch 8/40\n",
      "167/167 [==============================] - 23s 136ms/step - loss: 0.5200 - accuracy: 0.7763 - f1_m: 0.7111 - val_loss: 0.5009 - val_accuracy: 0.7824 - val_f1_m: 0.5855\n",
      "Epoch 9/40\n",
      "167/167 [==============================] - 24s 141ms/step - loss: 0.5095 - accuracy: 0.7761 - f1_m: 0.7067 - val_loss: 0.4914 - val_accuracy: 0.7820 - val_f1_m: 0.5949\n",
      "Epoch 10/40\n",
      "167/167 [==============================] - 24s 146ms/step - loss: 0.5041 - accuracy: 0.7786 - f1_m: 0.7093 - val_loss: 0.4854 - val_accuracy: 0.7824 - val_f1_m: 0.5915\n",
      "Epoch 11/40\n",
      "167/167 [==============================] - 23s 137ms/step - loss: 0.4981 - accuracy: 0.7834 - f1_m: 0.7149 - val_loss: 0.4807 - val_accuracy: 0.7863 - val_f1_m: 0.5852\n",
      "Epoch 12/40\n",
      "167/167 [==============================] - 23s 140ms/step - loss: 0.4927 - accuracy: 0.7818 - f1_m: 0.7105 - val_loss: 0.4755 - val_accuracy: 0.7885 - val_f1_m: 0.5912\n",
      "Epoch 13/40\n",
      "167/167 [==============================] - 33s 200ms/step - loss: 0.4898 - accuracy: 0.7838 - f1_m: 0.7140 - val_loss: 0.4710 - val_accuracy: 0.7916 - val_f1_m: 0.5970\n",
      "Epoch 14/40\n",
      "167/167 [==============================] - 29s 173ms/step - loss: 0.4859 - accuracy: 0.7853 - f1_m: 0.7185 - val_loss: 0.4668 - val_accuracy: 0.7951 - val_f1_m: 0.6126\n",
      "Epoch 15/40\n",
      "167/167 [==============================] - 31s 186ms/step - loss: 0.4832 - accuracy: 0.7895 - f1_m: 0.7182 - val_loss: 0.4640 - val_accuracy: 0.7951 - val_f1_m: 0.6053\n",
      "Epoch 16/40\n",
      "167/167 [==============================] - 24s 142ms/step - loss: 0.4793 - accuracy: 0.7891 - f1_m: 0.7185 - val_loss: 0.4610 - val_accuracy: 0.7968 - val_f1_m: 0.6108\n",
      "Epoch 17/40\n",
      "167/167 [==============================] - 20s 120ms/step - loss: 0.4757 - accuracy: 0.7868 - f1_m: 0.7190 - val_loss: 0.4590 - val_accuracy: 0.7955 - val_f1_m: 0.6064\n",
      "Epoch 18/40\n",
      "167/167 [==============================] - 21s 127ms/step - loss: 0.4741 - accuracy: 0.7896 - f1_m: 0.7144 - val_loss: 0.4572 - val_accuracy: 0.7977 - val_f1_m: 0.6052\n",
      "Epoch 19/40\n",
      "167/167 [==============================] - 22s 131ms/step - loss: 0.4707 - accuracy: 0.7887 - f1_m: 0.7163 - val_loss: 0.4552 - val_accuracy: 0.7990 - val_f1_m: 0.6080\n",
      "Epoch 20/40\n",
      "167/167 [==============================] - 23s 135ms/step - loss: 0.4686 - accuracy: 0.7904 - f1_m: 0.7190 - val_loss: 0.4528 - val_accuracy: 0.7986 - val_f1_m: 0.6121\n",
      "Epoch 21/40\n",
      "167/167 [==============================] - 19s 111ms/step - loss: 0.4664 - accuracy: 0.7940 - f1_m: 0.7249 - val_loss: 0.4532 - val_accuracy: 0.7995 - val_f1_m: 0.6207\n",
      "Epoch 22/40\n",
      "167/167 [==============================] - 18s 109ms/step - loss: 0.4696 - accuracy: 0.7885 - f1_m: 0.7221 - val_loss: 0.4510 - val_accuracy: 0.7973 - val_f1_m: 0.5985\n",
      "Epoch 23/40\n",
      "167/167 [==============================] - 18s 107ms/step - loss: 0.4652 - accuracy: 0.7949 - f1_m: 0.7246 - val_loss: 0.4493 - val_accuracy: 0.7986 - val_f1_m: 0.6128\n",
      "Epoch 24/40\n",
      "167/167 [==============================] - 18s 106ms/step - loss: 0.4665 - accuracy: 0.7865 - f1_m: 0.7131 - val_loss: 0.4484 - val_accuracy: 0.7968 - val_f1_m: 0.6051\n",
      "Epoch 25/40\n",
      "167/167 [==============================] - 18s 108ms/step - loss: 0.4641 - accuracy: 0.7934 - f1_m: 0.7234 - val_loss: 0.4481 - val_accuracy: 0.8004 - val_f1_m: 0.6197\n",
      "Epoch 26/40\n",
      "167/167 [==============================] - 18s 107ms/step - loss: 0.4637 - accuracy: 0.7940 - f1_m: 0.7239 - val_loss: 0.4467 - val_accuracy: 0.7977 - val_f1_m: 0.6088\n",
      "Epoch 27/40\n",
      "167/167 [==============================] - 19s 114ms/step - loss: 0.4604 - accuracy: 0.7936 - f1_m: 0.7259 - val_loss: 0.4472 - val_accuracy: 0.8060 - val_f1_m: 0.6290\n",
      "Epoch 28/40\n",
      "167/167 [==============================] - 21s 128ms/step - loss: 0.4594 - accuracy: 0.7975 - f1_m: 0.7315 - val_loss: 0.4454 - val_accuracy: 0.7982 - val_f1_m: 0.6067\n",
      "Epoch 29/40\n",
      "167/167 [==============================] - 20s 121ms/step - loss: 0.4571 - accuracy: 0.7930 - f1_m: 0.7229 - val_loss: 0.4449 - val_accuracy: 0.7995 - val_f1_m: 0.6062\n",
      "Epoch 30/40\n",
      "167/167 [==============================] - 21s 123ms/step - loss: 0.4601 - accuracy: 0.7951 - f1_m: 0.7251 - val_loss: 0.4442 - val_accuracy: 0.8017 - val_f1_m: 0.6136\n",
      "Epoch 31/40\n",
      "167/167 [==============================] - 22s 133ms/step - loss: 0.4591 - accuracy: 0.7932 - f1_m: 0.7249 - val_loss: 0.4440 - val_accuracy: 0.8039 - val_f1_m: 0.6173\n",
      "Epoch 32/40\n",
      "167/167 [==============================] - 23s 135ms/step - loss: 0.4588 - accuracy: 0.7932 - f1_m: 0.7257 - val_loss: 0.4434 - val_accuracy: 0.8034 - val_f1_m: 0.6172\n",
      "Epoch 33/40\n",
      "167/167 [==============================] - 25s 149ms/step - loss: 0.4561 - accuracy: 0.7943 - f1_m: 0.7243 - val_loss: 0.4428 - val_accuracy: 0.8012 - val_f1_m: 0.6097\n",
      "Epoch 34/40\n",
      "167/167 [==============================] - 21s 123ms/step - loss: 0.4564 - accuracy: 0.7915 - f1_m: 0.7233 - val_loss: 0.4426 - val_accuracy: 0.8047 - val_f1_m: 0.6167\n",
      "Epoch 35/40\n",
      "167/167 [==============================] - 21s 128ms/step - loss: 0.4562 - accuracy: 0.7949 - f1_m: 0.7258 - val_loss: 0.4426 - val_accuracy: 0.8065 - val_f1_m: 0.6226\n",
      "Epoch 36/40\n",
      "167/167 [==============================] - 21s 123ms/step - loss: 0.4556 - accuracy: 0.7930 - f1_m: 0.7223 - val_loss: 0.4419 - val_accuracy: 0.8043 - val_f1_m: 0.6146\n",
      "Epoch 37/40\n",
      "167/167 [==============================] - 21s 124ms/step - loss: 0.4517 - accuracy: 0.7985 - f1_m: 0.7307 - val_loss: 0.4417 - val_accuracy: 0.8034 - val_f1_m: 0.6144\n",
      "Epoch 38/40\n",
      "167/167 [==============================] - 22s 134ms/step - loss: 0.4549 - accuracy: 0.7975 - f1_m: 0.7297 - val_loss: 0.4412 - val_accuracy: 0.8039 - val_f1_m: 0.6140\n",
      "Epoch 39/40\n",
      "167/167 [==============================] - 24s 142ms/step - loss: 0.4508 - accuracy: 0.7956 - f1_m: 0.7235 - val_loss: 0.4411 - val_accuracy: 0.8060 - val_f1_m: 0.6208\n",
      "Epoch 40/40\n",
      "167/167 [==============================] - 26s 156ms/step - loss: 0.4536 - accuracy: 0.7996 - f1_m: 0.7313 - val_loss: 0.4408 - val_accuracy: 0.8078 - val_f1_m: 0.6210\n",
      "0:14:57.115241\n"
     ]
    }
   ],
   "source": [
    "# Fit the model; obtain the model fit time\n",
    "start=datetime.now()\n",
    "history1 = model1.fit(X_train,Y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=30,\n",
    "                    validation_split=0.3\n",
    ")\n",
    "end=datetime.now()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Train Accuracy: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7956"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('Best Train Accuracy: ')\n",
    "best_model_train_accuracy = np.round(history1.history['accuracy'][np.argmin(history1.history['loss'])],4)\n",
    "best_model_train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Accuracy: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8078"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('Best Validation Accuracy: ')\n",
    "best_model_val_accuracy = np.round(history1.history['val_accuracy'][np.argmin(history1.history['val_loss'])],4)\n",
    "best_model_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Train f1: 0.7315\n",
      "Best val f1: 0.629\n"
     ]
    }
   ],
   "source": [
    "best_model_train_f1 = max(np.round(history1.history['f1_m'],4))\n",
    "print ('Best Train f1: {}'.format(best_model_train_f1))\n",
    "\n",
    "best_model_val_f1 = max(np.round(history1.history['val_f1_m'],4))\n",
    "print ('Best val f1: {}'.format(best_model_val_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curve\n",
    "pd.DataFrame(history1.history).plot(figsize=(10, 8))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.title('Learning curve -  factor1: 400K, factor2: GloVe.6B.50d, factor3: LSTM, factor4: 0.1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 31)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(test_text)\n",
    "padded_seq = keras.preprocessing.sequence.pad_sequences(test_sequences,maxlen)\n",
    "X_test = padded_seq\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the prediction on the test data\n",
    "# 1 - disaster tweet , 0 - non-disaster tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict1 = model1.predict_classes(X_test)\n",
    "predict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results to a csv\n",
    "submission_df = pd.DataFrame({'id':test_tweets['id'],'target':np.squeeze(predict1)})\n",
    "submission_df.to_csv('submission_rnn_1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model_name</th>\n",
       "      <th>vocabulary size (factor1)</th>\n",
       "      <th>pre-trained vector (factor2)</th>\n",
       "      <th>RNN structure (factor3)</th>\n",
       "      <th>hyperparameter recurrent_dropout (factor4)</th>\n",
       "      <th>Processing Time</th>\n",
       "      <th>Training Set Accuracy</th>\n",
       "      <th>Validation Set Accuracy</th>\n",
       "      <th>train f1</th>\n",
       "      <th>validation f1</th>\n",
       "      <th>Test Set F1-score (Kaggle score)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RNN #1</td>\n",
       "      <td>400K</td>\n",
       "      <td>GloVe.6B.50d</td>\n",
       "      <td>LSTM layer based</td>\n",
       "      <td>0.1</td>\n",
       "      <td>00:14:57.115241</td>\n",
       "      <td>0.7956</td>\n",
       "      <td>0.8078</td>\n",
       "      <td>0.7315</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.78087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model_name vocabulary size (factor1) pre-trained vector (factor2)  \\\n",
       "0     RNN #1                      400K                 GloVe.6B.50d   \n",
       "\n",
       "  RNN structure (factor3) hyperparameter recurrent_dropout (factor4)  \\\n",
       "0        LSTM layer based                                        0.1   \n",
       "\n",
       "  Processing Time  Training Set Accuracy  Validation Set Accuracy  train f1  \\\n",
       "0 00:14:57.115241                 0.7956                   0.8078    0.7315   \n",
       "\n",
       "   validation f1 Test Set F1-score (Kaggle score)  \n",
       "0          0.629                          0.78087  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Store the accuracy and kaggle score for this model \n",
    "results_tbl = results_tbl.append({'Model_name':'RNN #1', \n",
    "                                   'vocabulary size (factor1)':'400K',\n",
    "                                   'pre-trained vector (factor2)':'GloVe.6B.50d',\n",
    "                                   'RNN structure (factor3)':'LSTM layer based',\n",
    "                                   'hyperparameter recurrent_dropout (factor4)': '0.1',\n",
    "                                   'Processing Time':(end-start),\n",
    "                                   'Training Set Accuracy':best_model_train_accuracy,\n",
    "                                   'Validation Set Accuracy':best_model_val_accuracy,\n",
    "                                   'train f1':best_model_train_f1,\n",
    "                                   'validation f1':best_model_val_f1,\n",
    "                                   'Test Set F1-score (Kaggle score)': '0.78087'\n",
    "                                   },ignore_index=True)\n",
    "results_tbl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit RNN 2\n",
    "# vocabulary size (factor 1): 400K\n",
    "# pre-trained vector (factor 2): GloVe.6B.50d\n",
    "# rnn structure (factor 3): LSTM layer based structure\n",
    "# recurrent dropout (factor 4): 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.models.Sequential([\n",
    "    keras.layers.Embedding(len(word2index)+1,50,weights=[embedding_matrix],input_length=maxlen,trainable=False),\n",
    "    keras.layers.LSTM(units=128 , return_sequences = True , recurrent_dropout = 0.3 , dropout = 0.3),\n",
    "    keras.layers.LSTM(units=64 , recurrent_dropout = 0.3 , dropout = 0.3),\n",
    "    keras.layers.Dense(32, activation='tanh'),\n",
    "    keras.layers.Dense(1,activation='tanh')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 31, 50)            612850    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 31, 128)           91648     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 756,019\n",
      "Trainable params: 143,169\n",
      "Non-trainable params: 612,850\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=Adam(learning_rate=1e-5)\n",
    "model2.compile(\n",
    "  loss='binary_crossentropy',\n",
    "  optimizer=opt,\n",
    "  metrics=['accuracy', f1_m],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "167/167 [==============================] - 24s 142ms/step - loss: 1.2531 - accuracy: 0.5752 - f1_m: 5.7029e-04 - val_loss: 0.9155 - val_accuracy: 0.5582 - val_f1_m: 0.0000e+00\n",
      "Epoch 2/40\n",
      "167/167 [==============================] - 22s 134ms/step - loss: 0.9891 - accuracy: 0.5744 - f1_m: 0.0088 - val_loss: 0.8608 - val_accuracy: 0.5582 - val_f1_m: 0.0012\n",
      "Epoch 3/40\n",
      "167/167 [==============================] - 21s 125ms/step - loss: 0.8963 - accuracy: 0.5648 - f1_m: 0.0207 - val_loss: 0.8180 - val_accuracy: 0.5539 - val_f1_m: 0.0189\n",
      "Epoch 4/40\n",
      "167/167 [==============================] - 21s 125ms/step - loss: 0.8444 - accuracy: 0.5600 - f1_m: 0.0617 - val_loss: 0.7833 - val_accuracy: 0.5429 - val_f1_m: 0.0893\n",
      "Epoch 5/40\n",
      "167/167 [==============================] - 22s 133ms/step - loss: 0.8117 - accuracy: 0.5594 - f1_m: 0.1263 - val_loss: 0.7633 - val_accuracy: 0.5429 - val_f1_m: 0.1761\n",
      "Epoch 6/40\n",
      "167/167 [==============================] - 24s 141ms/step - loss: 0.8063 - accuracy: 0.5558 - f1_m: 0.1751 - val_loss: 0.7516 - val_accuracy: 0.5355 - val_f1_m: 0.2139\n",
      "Epoch 7/40\n",
      "167/167 [==============================] - 24s 143ms/step - loss: 0.7699 - accuracy: 0.5498 - f1_m: 0.2097 - val_loss: 0.7367 - val_accuracy: 0.5468 - val_f1_m: 0.2851\n",
      "Epoch 8/40\n",
      "167/167 [==============================] - 24s 142ms/step - loss: 0.7631 - accuracy: 0.5564 - f1_m: 0.2474 - val_loss: 0.7256 - val_accuracy: 0.5591 - val_f1_m: 0.3265\n",
      "Epoch 9/40\n",
      "167/167 [==============================] - 23s 140ms/step - loss: 0.7491 - accuracy: 0.5577 - f1_m: 0.2804 - val_loss: 0.7166 - val_accuracy: 0.5661 - val_f1_m: 0.3539\n",
      "Epoch 10/40\n",
      "167/167 [==============================] - 23s 138ms/step - loss: 0.7367 - accuracy: 0.5635 - f1_m: 0.3117 - val_loss: 0.7069 - val_accuracy: 0.5683 - val_f1_m: 0.3679\n",
      "Epoch 11/40\n",
      "167/167 [==============================] - 22s 132ms/step - loss: 0.7246 - accuracy: 0.5744 - f1_m: 0.3530 - val_loss: 0.6930 - val_accuracy: 0.5823 - val_f1_m: 0.4026\n",
      "Epoch 12/40\n",
      "167/167 [==============================] - 21s 124ms/step - loss: 0.7100 - accuracy: 0.5731 - f1_m: 0.3594 - val_loss: 0.6835 - val_accuracy: 0.6011 - val_f1_m: 0.4212\n",
      "Epoch 13/40\n",
      "167/167 [==============================] - 23s 136ms/step - loss: 0.6983 - accuracy: 0.5965 - f1_m: 0.3858 - val_loss: 0.6742 - val_accuracy: 0.6187 - val_f1_m: 0.4373\n",
      "Epoch 14/40\n",
      "167/167 [==============================] - 21s 124ms/step - loss: 0.6823 - accuracy: 0.5905 - f1_m: 0.3833 - val_loss: 0.6643 - val_accuracy: 0.6349 - val_f1_m: 0.4481\n",
      "Epoch 15/40\n",
      "167/167 [==============================] - 20s 123ms/step - loss: 0.6902 - accuracy: 0.6168 - f1_m: 0.4283 - val_loss: 0.6537 - val_accuracy: 0.6497 - val_f1_m: 0.4479\n",
      "Epoch 16/40\n",
      "167/167 [==============================] - 21s 123ms/step - loss: 0.6781 - accuracy: 0.6268 - f1_m: 0.4304 - val_loss: 0.6429 - val_accuracy: 0.6725 - val_f1_m: 0.4641\n",
      "Epoch 17/40\n",
      "167/167 [==============================] - 20s 123ms/step - loss: 0.6722 - accuracy: 0.6446 - f1_m: 0.4679 - val_loss: 0.6320 - val_accuracy: 0.6896 - val_f1_m: 0.4803\n",
      "Epoch 18/40\n",
      "167/167 [==============================] - 20s 122ms/step - loss: 0.6561 - accuracy: 0.6583 - f1_m: 0.4734 - val_loss: 0.6198 - val_accuracy: 0.7053 - val_f1_m: 0.4931\n",
      "Epoch 19/40\n",
      "167/167 [==============================] - 20s 121ms/step - loss: 0.6509 - accuracy: 0.6701 - f1_m: 0.4974 - val_loss: 0.6090 - val_accuracy: 0.7172 - val_f1_m: 0.5025\n",
      "Epoch 20/40\n",
      "167/167 [==============================] - 20s 122ms/step - loss: 0.6354 - accuracy: 0.6767 - f1_m: 0.5308 - val_loss: 0.6059 - val_accuracy: 0.7150 - val_f1_m: 0.5384\n",
      "Epoch 21/40\n",
      "167/167 [==============================] - 21s 124ms/step - loss: 0.6317 - accuracy: 0.6763 - f1_m: 0.5431 - val_loss: 0.5967 - val_accuracy: 0.7272 - val_f1_m: 0.5441\n",
      "Epoch 22/40\n",
      "167/167 [==============================] - 20s 120ms/step - loss: 0.6281 - accuracy: 0.6958 - f1_m: 0.5566 - val_loss: 0.5867 - val_accuracy: 0.7377 - val_f1_m: 0.5464\n",
      "Epoch 23/40\n",
      "167/167 [==============================] - 20s 119ms/step - loss: 0.6068 - accuracy: 0.7048 - f1_m: 0.5762 - val_loss: 0.5773 - val_accuracy: 0.7395 - val_f1_m: 0.5373\n",
      "Epoch 24/40\n",
      "167/167 [==============================] - 20s 119ms/step - loss: 0.6064 - accuracy: 0.7095 - f1_m: 0.5824 - val_loss: 0.5725 - val_accuracy: 0.7426 - val_f1_m: 0.5437\n",
      "Epoch 25/40\n",
      "167/167 [==============================] - 20s 119ms/step - loss: 0.5980 - accuracy: 0.7146 - f1_m: 0.5923 - val_loss: 0.5631 - val_accuracy: 0.7434 - val_f1_m: 0.5431\n",
      "Epoch 26/40\n",
      "167/167 [==============================] - 20s 117ms/step - loss: 0.5948 - accuracy: 0.7219 - f1_m: 0.6131 - val_loss: 0.5600 - val_accuracy: 0.7500 - val_f1_m: 0.5665\n",
      "Epoch 27/40\n",
      "167/167 [==============================] - 20s 119ms/step - loss: 0.5920 - accuracy: 0.7311 - f1_m: 0.6265 - val_loss: 0.5531 - val_accuracy: 0.7496 - val_f1_m: 0.5629\n",
      "Epoch 28/40\n",
      "167/167 [==============================] - 20s 118ms/step - loss: 0.5816 - accuracy: 0.7318 - f1_m: 0.6374 - val_loss: 0.5508 - val_accuracy: 0.7509 - val_f1_m: 0.5711\n",
      "Epoch 29/40\n",
      "167/167 [==============================] - 20s 117ms/step - loss: 0.5865 - accuracy: 0.7298 - f1_m: 0.6394 - val_loss: 0.5478 - val_accuracy: 0.7548 - val_f1_m: 0.5698\n",
      "Epoch 30/40\n",
      "167/167 [==============================] - 20s 118ms/step - loss: 0.5735 - accuracy: 0.7403 - f1_m: 0.6530 - val_loss: 0.5470 - val_accuracy: 0.7557 - val_f1_m: 0.5842\n",
      "Epoch 31/40\n",
      "167/167 [==============================] - 20s 117ms/step - loss: 0.5767 - accuracy: 0.7382 - f1_m: 0.6533 - val_loss: 0.5446 - val_accuracy: 0.7566 - val_f1_m: 0.5882\n",
      "Epoch 32/40\n",
      "167/167 [==============================] - 20s 117ms/step - loss: 0.5716 - accuracy: 0.7394 - f1_m: 0.6623 - val_loss: 0.5361 - val_accuracy: 0.7601 - val_f1_m: 0.5851\n",
      "Epoch 33/40\n",
      "167/167 [==============================] - 19s 116ms/step - loss: 0.5564 - accuracy: 0.7452 - f1_m: 0.6624 - val_loss: 0.5324 - val_accuracy: 0.7609 - val_f1_m: 0.5843\n",
      "Epoch 34/40\n",
      "153/167 [==========================>...] - ETA: 1s - loss: 0.5690 - accuracy: 0.7296 - f1_m: 0.6687"
     ]
    }
   ],
   "source": [
    "# Fit the model; obtain the model fit time\n",
    "start=datetime.now()\n",
    "history2 = model2.fit(X_train,Y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=40,\n",
    "                    validation_split=0.3\n",
    ")\n",
    "end=datetime.now()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Best Train Accuracy: ')\n",
    "best_model_train_accuracy = np.round(history2.history['accuracy'][np.argmin(history2.history['loss'])],4)\n",
    "best_model_train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Best Validation Accuracy: ')\n",
    "best_model_val_accuracy = np.round(history2.history['val_accuracy'][np.argmin(history2.history['val_loss'])],4)\n",
    "best_model_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_train_f1 = max(np.round(history2.history['f1_m'],4))\n",
    "print ('Best Train f1: {}'.format(best_model_train_f1))\n",
    "\n",
    "best_model_val_f1 = max(np.round(history2.history['val_f1_m'],4))\n",
    "print ('Best val f1: {}'.format(best_model_val_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_train_f1 = max(np.round(history2.history['f1_m'],4))\n",
    "print ('Best Train f1: {}'.format(best_model_train_f1))\n",
    "\n",
    "best_model_val_f1 = max(np.round(history2.history['val_f1_m'],4))\n",
    "print ('Best val f1: {}'.format(best_model_val_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curve\n",
    "pd.DataFrame(history2.history).plot(figsize=(10, 8))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.title('Learning curve -  factor1: 400K, factor2: GloVe.6B.50d, factor3: LSTM, factor4: 0.3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-use the X_test obtained earlier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the prediction on the test data\n",
    "# 1 - disaster tweet , 0 - non-disaster tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict2 = model2.predict_classes(X_test)\n",
    "predict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results to a csv\n",
    "submission_df = pd.DataFrame({'id':test_tweets['id'],'target':np.squeeze(predict2)})\n",
    "submission_df.to_csv('submission_rnn_2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the accuracy and kaggle score for this model \n",
    "results_tbl = results_tbl.append({'Model_name':'RNN #2', \n",
    "                                   'vocabulary size (factor1)':'400K',\n",
    "                                   'pre-trained vector (factor2)':'GloVe.6B.50d',\n",
    "                                   'RNN structure (factor3)':'LSTM layer based',\n",
    "                                   'hyperparameter recurrent_dropout (factor4)': '0.3',\n",
    "                                   'Processing Time':(end-start),\n",
    "                                   'Training Set Accuracy':best_model_train_accuracy,\n",
    "                                   'Validation Set Accuracy':best_model_val_accuracy,\n",
    "                                   'train f1':best_model_train_f1,\n",
    "                                   'validation f1':best_model_val_f1,\n",
    "                                   'Test Set F1-score (Kaggle score)': '0.78026'\n",
    "                                   },ignore_index=True)\n",
    "results_tbl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit RNN 3\n",
    "# vocabulary size (factor 1): 400K\n",
    "# pre-trained vector (factor 2): GloVe.6B.50d\n",
    "# rnn structure (factor 3): GRU layer based structure\n",
    "# recurrent dropout (factor 4): 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = keras.models.Sequential([\n",
    "    keras.layers.Embedding(len(word2index)+1,50,weights=[embedding_matrix],input_length=maxlen,trainable=False),\n",
    "    keras.layers.GRU(units=128 , return_sequences = True , recurrent_dropout = 0.1 , dropout = 0.1),\n",
    "    keras.layers.GRU(units=64 , recurrent_dropout = 0.1 , dropout = 0.1),\n",
    "    keras.layers.Dense(32, activation='tanh'),\n",
    "    keras.layers.Dense(1,activation='tanh')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=Adam(learning_rate=1e-5)\n",
    "model3.compile(\n",
    "  loss='binary_crossentropy',\n",
    "  optimizer=opt,\n",
    "  metrics=['accuracy', f1_m],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model; obtain the model fit time\n",
    "start=datetime.now()\n",
    "history3 = model3.fit(X_train,Y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=40,\n",
    "                    validation_split=0.3\n",
    ")\n",
    "end=datetime.now()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Best Train Accuracy: ')\n",
    "best_model_train_accuracy = np.round(history3.history['accuracy'][np.argmin(history3.history['loss'])],4)\n",
    "best_model_train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Best Validation Accuracy: ')\n",
    "best_model_val_accuracy = np.round(history3.history['val_accuracy'][np.argmin(history3.history['val_loss'])],4)\n",
    "best_model_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_train_f1 = max(np.round(history3.history['f1_m'],4))\n",
    "print ('Best Train f1: {}'.format(best_model_train_f1))\n",
    "\n",
    "best_model_val_f1 = max(np.round(history3.history['val_f1_m'],4))\n",
    "print ('Best val f1: {}'.format(best_model_val_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curve\n",
    "pd.DataFrame(history3.history).plot(figsize=(10, 8))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.title('Learning curve - factor1: 400K, factor2: GloVe.6B.50d, factor3: GRU, factor4: 0.1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-use the X_test obtained earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the prediction on the test data\n",
    "# 1 - disaster tweet , 0 - non-disaster tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict3 = model3.predict_classes(X_test)\n",
    "predict3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predict3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results to a csv\n",
    "submission_df = pd.DataFrame({'id':test_tweets['id'],'target':np.squeeze(predict3)})\n",
    "submission_df.to_csv('submission_rnn_3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the accuracy and kaggle score for this model \n",
    "results_tbl = results_tbl.append({'Model_name':'RNN #3', \n",
    "                                   'vocabulary size (factor1)':'400K',\n",
    "                                   'pre-trained vector (factor2)':'GloVe.6B.50d',\n",
    "                                   'RNN structure (factor3)':'GRU layer based',\n",
    "                                   'hyperparameter recurrent_dropout (factor4)': '0.1',\n",
    "                                   'Processing Time':(end-start),\n",
    "                                   'Training Set Accuracy':best_model_train_accuracy,\n",
    "                                   'Validation Set Accuracy':best_model_val_accuracy,\n",
    "                                   'train f1':best_model_train_f1,\n",
    "                                   'validation f1':best_model_val_f1,\n",
    "                                   'Test Set F1-score (Kaggle score)': '0.78302'\n",
    "                                   },ignore_index=True)\n",
    "results_tbl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit RNN 4\n",
    "# vocabulary size (factor 1): 400K\n",
    "# pre-trained vector (factor 2): GloVe.6B.50d\n",
    "# rnn structure (factor 3): GRU based structure\n",
    "# recurrent dropout (factor 4): 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = keras.models.Sequential([\n",
    "    keras.layers.Embedding(len(word2index)+1,50,weights=[embedding_matrix],input_length=maxlen,trainable=False),\n",
    "    keras.layers.GRU(units=128 , return_sequences = True , recurrent_dropout = 0.3 , dropout = 0.3),\n",
    "    keras.layers.GRU(units=64 , recurrent_dropout = 0.3 , dropout = 0.3),\n",
    "    keras.layers.Dense(32, activation='tanh'),\n",
    "    keras.layers.Dense(1,activation='tanh')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=Adam(learning_rate=1e-5)\n",
    "model4.compile(\n",
    "  loss='binary_crossentropy',\n",
    "  optimizer=opt,\n",
    "  metrics=['accuracy', f1_m],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model; obtain the model fit time\n",
    "start=datetime.now()\n",
    "history4 = model4.fit(X_train,Y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=40,\n",
    "                    validation_split=0.3\n",
    ")\n",
    "end=datetime.now()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Best Train Accuracy: ')\n",
    "best_model_train_accuracy = np.round(history4.history['accuracy'][np.argmin(history4.history['loss'])],4)\n",
    "best_model_train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Best Validation Accuracy: ')\n",
    "best_model_val_accuracy = np.round(history4.history['val_accuracy'][np.argmin(history4.history['val_loss'])],4)\n",
    "best_model_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_train_f1 = max(np.round(history4.history['f1_m'],4))\n",
    "print ('Best Train f1: {}'.format(best_model_train_f1))\n",
    "\n",
    "best_model_val_f1 = max(np.round(history4.history['val_f1_m'],4))\n",
    "print ('Best val f1: {}'.format(best_model_val_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curve\n",
    "pd.DataFrame(history4.history).plot(figsize=(10, 8))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.title('Learning curve - factor1: 400K, factor2: GloVe.6B.50d, factor3: GRU, factor4: 0.3')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-use the X_test obtained earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the prediction on the test data\n",
    "# 1 - disaster tweet , 0 - non-disaster tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict4 = model4.predict_classes(X_test)\n",
    "predict4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results to a csv\n",
    "submission_df = pd.DataFrame({'id':test_tweets['id'],'target':np.squeeze(predict4)})\n",
    "submission_df.to_csv('submission_rnn_4.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Store the accuracy and kaggle score for this model \n",
    "results_tbl = results_tbl.append({'Model_name':'RNN #4', \n",
    "                                   'vocabulary size (factor1)':'400K',\n",
    "                                   'pre-trained vector (factor2)':'GloVe.6B.50d',\n",
    "                                   'RNN structure (factor3)':'GRU layer based',\n",
    "                                   'hyperparameter recurrent_dropout (factor4)': '0.3',\n",
    "                                   'Processing Time':(end-start),\n",
    "                                   'Training Set Accuracy':best_model_train_accuracy,\n",
    "                                   'Validation Set Accuracy':best_model_val_accuracy,\n",
    "                                   'train f1':best_model_train_f1,\n",
    "                                   'validation f1':best_model_val_f1,\n",
    "                                   'Test Set F1-score (Kaggle score)': '0.77873'\n",
    "                                   },ignore_index=True)\n",
    "results_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Glove file - gloVe.6B.200d.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading from the file to learn the word embedding into the list word_to_vec_map\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('/Users/harini-mac/Desktop/Northwestern University/MSDS-422/Week8/project1/nlp-getting-started/embeddings/gloVe.6B/glove.6B.200d.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_to_vec_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(index_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanse the training and test data \n",
    "## Use the word_to_vec_map from glove.6B.200d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the training and test data using the glove.6B.200d word_to_vec_map\n",
    "for i in range (train_tweets.shape[0]):\n",
    "    train_tweets.at[i,'text']=clean(train_tweets.loc[i,'text'])\n",
    "    \n",
    "for i in range(test_tweets.shape[0]):\n",
    "    test_tweets.at[i,'text']=clean(test_tweets.loc[i,'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train_tweets['text']\n",
    "test_text = test_tweets['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode the text in the training data\n",
    "# Pad the resultant sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.texts_to_sequences([\"Forest fire near La Ronge Sask. Canada\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.sequences_to_texts([[178, 42, 214, 664, 6037, 6038, 1319]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = tokenizer.word_index\n",
    "print(\"Number of unique tokens : \",len(word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_size = tokenizer.document_count\n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get max training sequence length\n",
    "maxlen = max([len(x) for x in train_sequences])\n",
    "maxlen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_seq = keras.preprocessing.sequence.pad_sequences(train_sequences,maxlen)\n",
    "X_train = padded_seq\n",
    "Y_train = train_tweets['target']\n",
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate embedding matrix for tokens in the training data - 200 Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word2index)+1,200))\n",
    "\n",
    "embedding_vec=[]\n",
    "for word, i in word2index.items():\n",
    "    embedding_vec = word_to_vec_map.get(word)\n",
    "    if embedding_vec is not None:\n",
    "        embedding_matrix[i] = embedding_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_matrix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit RNN 5\n",
    "# vocabulary size (factor 1): 400K\n",
    "# pre-trained vector (factor 2): GloVe.6B.200d\n",
    "# rnn structure (factor 3): LSTM layer based structure\n",
    "# recurrent dropout (factor 4): 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = keras.models.Sequential([\n",
    "    keras.layers.Embedding(len(word2index)+1,200,weights=[embedding_matrix],input_length=maxlen,trainable=False),\n",
    "    keras.layers.LSTM(units=128 , return_sequences = True , recurrent_dropout = 0.1 , dropout = 0.1),\n",
    "    keras.layers.LSTM(units=64 , recurrent_dropout = 0.1 , dropout = 0.1),\n",
    "    keras.layers.Dense(32, activation='tanh'),\n",
    "    keras.layers.Dense(1,activation='tanh')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=Adam(learning_rate=1e-5)\n",
    "model5.compile(\n",
    "  loss='binary_crossentropy',\n",
    "  optimizer=opt,\n",
    "  metrics=['accuracy', f1_m],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model; obtain the model fit time\n",
    "start=datetime.now()\n",
    "history5 = model5.fit(X_train,Y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=40,\n",
    "                    validation_split=0.1\n",
    ")\n",
    "end=datetime.now()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Best Train Accuracy: ')\n",
    "best_model_train_accuracy = np.round(history5.history['accuracy'][np.argmin(history5.history['loss'])],4)\n",
    "best_model_train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Best Validation Accuracy: ')\n",
    "best_model_val_accuracy = np.round(history5.history['val_accuracy'][np.argmin(history5.history['val_loss'])],4)\n",
    "best_model_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_train_f1 = max(np.round(history5.history['f1_m'],4))\n",
    "print ('Best Train f1: {}'.format(best_model_train_f1))\n",
    "\n",
    "best_model_val_f1 = max(np.round(history5.history['val_f1_m'],4))\n",
    "print ('Best val f1: {}'.format(best_model_val_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the learning curve\n",
    "pd.DataFrame(history5.history).plot(figsize=(10, 8))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.title('Learning curve - factor1: 400K, factor2: GloVe.6B.200d, factor3: LSTM, factor4: 0.1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(test_text)\n",
    "padded_seq = keras.preprocessing.sequence.pad_sequences(test_sequences,maxlen)\n",
    "X_test = padded_seq\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predict5 = model5.predict_classes(X_test)\n",
    "predict5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predict5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results to a csv\n",
    "submission_df = pd.DataFrame({'id':test_tweets['id'],'target':np.squeeze(predict5)})\n",
    "submission_df.to_csv('submission_rnn_5.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the accuracy and kaggle score for this model \n",
    "results_tbl = results_tbl.append({'Model_name':'RNN #5', \n",
    "                                   'vocabulary size (factor1)':'400K',\n",
    "                                   'pre-trained vector (factor2)':'GloVe.6B.200d',\n",
    "                                   'RNN structure (factor3)':'LSTM layer based',\n",
    "                                   'hyperparameter recurrent_dropout (factor4)': '0.1',\n",
    "                                   'Processing Time':(end-start),\n",
    "                                   'Training Set Accuracy':best_model_train_accuracy,\n",
    "                                   'Validation Set Accuracy':best_model_val_accuracy,\n",
    "                                   'train f1':best_model_train_f1,\n",
    "                                   'validation f1':best_model_val_f1,\n",
    "                                   'Test Set F1-score (Kaggle score)': '0.79313'\n",
    "                                   },ignore_index=True)\n",
    "results_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit RNN 6\n",
    "# vocabulary size (factor 1): 400K\n",
    "# pre-trained vector (factor 2): GloVe.6B.200d\n",
    "# rnn structure (factor 3): LSTM layer based structure\n",
    "# recurrent dropout (factor 4): 0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = keras.models.Sequential([\n",
    "    keras.layers.Embedding(len(word2index)+1,200,weights=[embedding_matrix],input_length=maxlen,trainable=False),\n",
    "    keras.layers.LSTM(units=128 , return_sequences = True , recurrent_dropout = 0.3 , dropout = 0.3),\n",
    "    keras.layers.LSTM(units=64 , recurrent_dropout = 0.3 , dropout = 0.3),\n",
    "    keras.layers.Dense(32, activation='tanh'),\n",
    "    keras.layers.Dense(1,activation='tanh')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=Adam(learning_rate=1e-5)\n",
    "model6.compile(\n",
    "  loss='binary_crossentropy',\n",
    "  optimizer=opt,\n",
    "  metrics=['accuracy', f1_m],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model; obtain the model fit time\n",
    "start=datetime.now()\n",
    "history6 = model6.fit(X_train,Y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=40,\n",
    "                    validation_split=0.3\n",
    ")\n",
    "end=datetime.now()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Best Train Accuracy: ')\n",
    "best_model_train_accuracy = np.round(history6.history['accuracy'][np.argmin(history6.history['loss'])],4)\n",
    "best_model_train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Best Validation Accuracy: ')\n",
    "best_model_val_accuracy = np.round(history6.history['val_accuracy'][np.argmin(history6.history['val_loss'])],4)\n",
    "best_model_val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_train_f1 = max(np.round(history6.history['f1_m'],4))\n",
    "print ('Best Train f1: {}'.format(best_model_train_f1))\n",
    "\n",
    "best_model_val_f1 = max(np.round(history6.history['val_f1_m'],4))\n",
    "print ('Best val f1: {}'.format(best_model_val_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the learning curve\n",
    "pd.DataFrame(history6.history).plot(figsize=(10, 8))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.title('Learning curve - factor1: 400K, factor2: GloVe.6B.200d, factor3: LSTM, factor4: 0.3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-use the X_test obtained earlier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the prediction on the test data\n",
    "# 1 - disaster tweet , 0 - non-disaster tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predict6 = model6.predict_classes(X_test)\n",
    "predict6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predict6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results to a csv\n",
    "submission_df = pd.DataFrame({'id':test_tweets['id'],'target':np.squeeze(predict6)})\n",
    "submission_df.to_csv('submission_rnn_6.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the accuracy and kaggle score for this model \n",
    "results_tbl = results_tbl.append({'Model_name':'RNN #6', \n",
    "                                   'vocabulary size (factor1)':'400K',\n",
    "                                   'pre-trained vector (factor2)':'GloVe.6B.200d',\n",
    "                                   'RNN structure (factor3)':'LSTM layer based',\n",
    "                                   'hyperparameter recurrent_dropout (factor4)': '0.3',\n",
    "                                   'Processing Time':(end-start),\n",
    "                                   'Training Set Accuracy':best_model_train_accuracy,\n",
    "                                   'Validation Set Accuracy':best_model_val_accuracy,\n",
    "                                   'train f1':best_model_train_f1,\n",
    "                                   'validation f1':best_model_val_f1,\n",
    "                                   'Test Set F1-score (Kaggle score)': '0.78639'\n",
    "                                   },ignore_index=True)\n",
    "results_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit RNN 7\n",
    "# vocabulary size (factor 1): 400K\n",
    "# pre-trained vector (factor 2): GloVe.6B.200d\n",
    "# rnn structure (factor 3): GRU layer based structure\n",
    "# recurrent dropout (factor 4): 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model7 = keras.models.Sequential([\n",
    "    keras.layers.Embedding(len(word2index)+1,200,weights=[embedding_matrix],input_length=maxlen,trainable=False),\n",
    "    keras.layers.GRU(units=128 , return_sequences = True , recurrent_dropout = 0.1 , dropout = 0.1),\n",
    "    keras.layers.GRU(units=64 , recurrent_dropout = 0.1 , dropout = 0.1),\n",
    "    keras.layers.Dense(32, activation='tanh'),\n",
    "    keras.layers.Dense(1,activation='tanh')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=Adam(learning_rate=1e-5)\n",
    "model7.compile(\n",
    "  loss='binary_crossentropy',\n",
    "  optimizer=opt,\n",
    "  metrics=['accuracy', f1_m],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model; obtain the model fit time\n",
    "start=datetime.now()\n",
    "history7 = model7.fit(X_train,Y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=40,\n",
    "                    validation_split=0.3\n",
    ")\n",
    "end=datetime.now()\n",
    "print(end-start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print ('Best Train Accuracy: ')\n",
    "best_model_train_accuracy = np.round(history7.history['accuracy'][np.argmin(history7.history['loss'])],4)\n",
    "best_model_train_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Best Validation Accuracy: ')\n",
    "best_model_val_accuracy = np.round(history7.history['val_accuracy'][np.argmin(history7.history['val_loss'])],4)\n",
    "best_model_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_train_f1 = max(np.round(history7.history['f1_m'],4))\n",
    "print ('Best Train f1: {}'.format(best_model_train_f1))\n",
    "\n",
    "best_model_val_f1 = max(np.round(history7.history['val_f1_m'],4))\n",
    "print ('Best val f1: {}'.format(best_model_val_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curve\n",
    "pd.DataFrame(history7.history).plot(figsize=(10, 8))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.title('Learning curve - factor1: 400K, factor2: GloVe.6B.200d, factor3: GRU, factor4: 0.1')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-use the X_test obtained earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the prediction on the test data\n",
    "\n",
    "# 1 - disaster tweet , 0 - non-disaster tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predict7 = model7.predict_classes(X_test)\n",
    "predict7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predict7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results to a csv\n",
    "submission_df = pd.DataFrame({'id':test_tweets['id'],'target':np.squeeze(predict7)})\n",
    "submission_df.to_csv('submission_rnn_7.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the accuracy and kaggle score for this model \n",
    "results_tbl = results_tbl.append({'Model_name':'RNN #7', \n",
    "                                   'vocabulary size (factor1)':'400K',\n",
    "                                   'pre-trained vector (factor2)':'GloVe.6B.200d',\n",
    "                                   'RNN structure (factor3)':'GRU layer based',\n",
    "                                   'hyperparameter recurrent_dropout (factor4)': '0.1',\n",
    "                                   'Processing Time':(end-start),\n",
    "                                   'Training Set Accuracy':best_model_train_accuracy,\n",
    "                                   'Validation Set Accuracy':best_model_val_accuracy,\n",
    "                                   'train f1':best_model_train_f1,\n",
    "                                   'validation f1':best_model_val_f1,\n",
    "                                   'Test Set F1-score (Kaggle score)': '0.79068'\n",
    "                                   },ignore_index=True)\n",
    "results_tbl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit RNN 8\n",
    "# vocabulary size (factor 1): 400K\n",
    "# pre-trained vector (factor 2): GloVe.6B.200d\n",
    "# rnn structure (factor 3): GRU based structure\n",
    "# recurrent dropout (factor 4): 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model8 = keras.models.Sequential([\n",
    "    keras.layers.Embedding(len(word2index)+1,200,weights=[embedding_matrix],input_length=maxlen,trainable=False),\n",
    "    keras.layers.GRU(units=128 , return_sequences = True , recurrent_dropout = 0.3 , dropout = 0.3),\n",
    "    keras.layers.GRU(units=64 , recurrent_dropout = 0.3 , dropout = 0.3),\n",
    "    keras.layers.Dense(32, activation='tanh'),\n",
    "    keras.layers.Dense(1,activation='tanh')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model8.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=Adam(learning_rate=1e-5)\n",
    "model8.compile(\n",
    "  loss='binary_crossentropy',\n",
    "  optimizer=opt,\n",
    "  metrics=['accuracy', f1_m],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fit the model; obtain the model fit time\n",
    "start=datetime.now()\n",
    "history8 = model8.fit(X_train,Y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=40,\n",
    "                    validation_split=0.3\n",
    ")\n",
    "end=datetime.now()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Best Train Accuracy: ')\n",
    "best_model_train_accuracy = np.round(history8.history['accuracy'][np.argmin(history8.history['loss'])],4)\n",
    "best_model_train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Best Validation Accuracy: ')\n",
    "best_model_val_accuracy = np.round(history8.history['val_accuracy'][np.argmin(history8.history['val_loss'])],4)\n",
    "best_model_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_train_f1 = max(np.round(history8.history['f1_m'],4))\n",
    "print ('Best Train f1: {}'.format(best_model_train_f1))\n",
    "\n",
    "best_model_val_f1 = max(np.round(history8.history['val_f1_m'],4))\n",
    "print ('Best val f1: {}'.format(best_model_val_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curve\n",
    "pd.DataFrame(history8.history).plot(figsize=(10, 8))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.title('Learning curve - factor1: 400K, factor2: GloVe.6B.200d, factor3: GRU, factor4: 0.3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-use the X_test obtained earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the prediction on the test data\n",
    "# 1 - disaster tweet , 0 - non-disaster tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict8 = model8.predict_classes(X_test)\n",
    "predict8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results to a csv\n",
    "submission_df = pd.DataFrame({'id':test_tweets['id'],'target':np.squeeze(predict8)})\n",
    "submission_df.to_csv('submission_rnn_8.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the accuracy and kaggle score for this model \n",
    "results_tbl = results_tbl.append({'Model_name':'RNN #8', \n",
    "                                   'vocabulary size (factor1)':'400K',\n",
    "                                   'pre-trained vector (factor2)':'GloVe.6B.200d',\n",
    "                                   'RNN structure (factor3)':'GRU layer based',\n",
    "                                   'hyperparameter recurrent_dropout (factor4)': '0.3',\n",
    "                                   'Processing Time':(end-start),\n",
    "                                   'Training Set Accuracy':best_model_train_accuracy,\n",
    "                                   'Validation Set Accuracy':best_model_val_accuracy,\n",
    "                                   'train f1':best_model_train_f1,\n",
    "                                   'validation f1':best_model_val_f1,\n",
    "                                   'Test Set F1-score (Kaggle score)': '0.78608'\n",
    "                                   },ignore_index=True)\n",
    "results_tbl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
